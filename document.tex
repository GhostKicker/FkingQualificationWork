\documentclass[a4paper,12pt,preview]{report} %размер бумаги устанавливаем А4, шрифт 12пунктов
\usepackage[english,russian]{babel}%используем русский и английский языки с переносами 	
\usepackage[T2A]{fontenc}
\usepackage{changepage}
\usepackage{lipsum}
\usepackage{indentfirst}
\usepackage[labelsep=period]{caption}
\usepackage{amsmath}
\usepackage[koi8-r]{inputenc}%включаем свою кодировку: koi8-r или utf8 в UNIX, cp1251 в Windows
\usepackage[english,russian]{babel}%используем русский и английский языки с переносами 	
\usepackage{amssymb,amsfonts,amsmath,mathtext,cite,enumerate,float} %подключаем нужные пакеты расширений
\usepackage{graphicx} %хотим вставлять в диплом рисунки?
\usepackage{indentfirst}
\graphicspath{{images/}}%п\usepackage{trd}уть к рисункам

\makeatletter
\renewcommand{\@biblabel}[1]{#1.} % Заменяем библиографию с квадратных скобок на точку:
\makeatother

\usepackage{geometry} % Меняем поля страницы
\geometry{left=2cm}% левое поле
\geometry{right=1.5cm}% правое поле
\geometry{top=1cm}% верхнее поле
\geometry{bottom=2cm}% нижнее поле

\renewcommand{\theenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumii}{.\arabic{enumii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}.}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumiii}{.\arabic{enumiii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.}% Меняем везде перечисления на цифра.цифра

\begin{document}
	\linespread{1.3}
	
	\tableofcontents % это оглавление, которое генерируется автоматически
	\chapter{Аннотация}
	
	бла-бла-бла
	
	
	
	
	\chapter{Аналитический обзор литературы}
	\section{Базовые методы и подходы}
	
	
	\textit{Текстовый корпус} -- большой и структурированный набор текстов (в настоящее время обычно хранится и обрабатывается в электронном виде). В корпусной лингвистике они используются для статистического анализа и проверки гипотез, проверки происшествий или языковых правил на определенной языковой территории.
	
	\textit{Регулярные выражения} (англ. regular expressions) - формальный язык поиска и осуществления манипуляций с подстроками в тексте, основанный на использовании метасимволов (символов-джокеров, англ. wildcard characters). Для поиска используется строка-образец (англ. pattern, по-русски её часто называют <<шаблоном>>, <<маской>>), состоящая из символов и метасимволов и задающая правило поиска. Для манипуляций с текстом дополнительно задаётся строка замены, которая также может содержать в себе специальные символы. [1]
	
	
	\textit{Парсер} (англ. parser; от parse -- анализ, разбор), или синтаксический анализатор, -- часть программы, преобразующей входные данные (как правило, текст) в структурированный формат. Парсер выполняет синтаксический анализ текста.
	
	\textit{Стемминг} - это процесс нахождения основы слова для заданного исходного слова. [2]
	
	\textit{Лемматизация} - процесс приведения словоформы к лемме - её нормальной (словарной) форме. [3]	
	
	\textit{Клитика} - это часть слова, которая не может стоять сама по себе и может появиться в тексте только когда оно прикреплено к другому слову. [4]
	
	\subsection{Регулярные выражения}
	Одним из невосполнимых успехов в стандартизации в информатике стали
	регулярные выражения (RE), язык для уточнения строк текстового поиска.	Этот язык используется во всех языках программирования, текстовых процессорах и инструментах обработки текста, таких как инструменты Unix grep или Emacs. Формально регулярное выражение - это	алгебраическое обозначение для характеристики набора строк. Они особенно полезны для поиска в текстах, когда есть шаблон для поиска и корпус	текста для поиска. Функция поиска по регулярному выражению будет искать через
	корпус, возвращая все тексты, которые соответствуют шаблону. Корпус может быть отдельным документом или коллекцией. Например, инструмент командной строки Unix grep принимает регулярное выражение и возвращает каждую строку входного документа, которая соответствует выражению.
	Поиск может быть спроектирован так, чтобы возвращать каждое совпадение в строке, если их больше
	одного или только самое первое совпадение.
	Регулярные выражения бывают разных видов. Далее будут описываться описывать расширенные регулярные выражения; различные парсеры регулярных выражений могут распознавать только их подмножества, или трактуют некоторые выражения немного по-другому. Использование онлайн-тестера регулярных выражений - это удобный способ проверить свои выражения и изучить эти варианты.
	
	\begin{center}
		\includegraphics[scale=0.2]{regex.png}
		\\ Рисунок 1 -- регулярные выражения в языке \\ программирования Python
	\end{center}
	
	
	\subsection{Нормализация текста}
	
	Перед обработкой текста практически на любом естественном языке текст должен быть нормализован. Как минимум три задачи входят в любой процесс нормализации:
	
	1. Токенизация (сегментирование) слов
	
	2. Нормализация форматов слов
	
	3. Сегментирование предложений
	
	Токенизатор также может быть использован для расширения клитикальных сокращений. 
	В зависимости от приложения алгоритмы токенизации могут также токенизировать выражения из нескольких слов, такие как <<New York>> или <<Rock 'n' Roll>>, как один токен, для чего требуется некоторый словарь выражений из нескольких слов. Таким образом, токенизация тесно связана с обнаружением именованных объектов, задачей обнаружения имен, дат и	организации.
	
	Один широко используемый стандарт токенизации известен как стандарт токенизации Penn Treebank, используемый для проанализированных корпусов (treebanks), выпущенных Linuistic Data Consortium (LDC), источником многих полезных наборов данных. Этот стандарт
	выделяет клитики, объединяет слова, написанные через дефисы, и выделяет все знаки препинания.
	На практике, поскольку токенизация должна выполняться перед любой другой языковой обработкой, она должна быть очень быстрой. Поэтому стандартный метод токенизации --
	использовать детерминированные алгоритмы, основанные на регулярных выражениях, скомпилированных в очень эффективные конечные автоматы.
	
	Токенизация в таких языках, как письменный китайский, японский и тайский, намного сложнее, так как они не используют пробелы для обозначения потенциальных границ слов.
	В китайском языке, например, слова состоят из символов (называемых ханзи). Каждый символ обычно представляет одну единицу значения (называется
	морфема) и произносится как один слог. Длина слов в среднем составляет около 2,4 символов. Но решить, что считать словом на китайском, сложно.
	
	
	Существует третий вариант токенизации текста. Вместо того, чтобы определять токены как слова
	или в виде символов (как на китайском), мы можем использовать данные для автоматического определения, какого размер токены должны быть. Возможно, иногда нам могут потребоваться токены -- разделенные пробелами
	слова; в других случаях полезно иметь токены размера больше, чем слова (например, New York Times), а иногда меньше, чем слова (например, морфема не-) Морфема - это наименьшая смысловая единица языка.
	
	\subsection{Byte Pair Encoding}
	
	Причина, по которой полезно иметь токены подслов - когда обращаешься с неизвестными словами.
	Неизвестные слова особенно актуальны для систем машинного обучения. Они часто узнают некоторые факты о словах
	в одном корпусе (учебный корпус), а затем используют эти факты для принятия решений слов в тестовом корпусе. Таким образом, если наш учебный корпус содержит, скажем,
	слова <<низкий>> и <<нижайший>>, но не <<ниже>>, но слово <<ниже>> появляется в тестовом
	корпусе, система не будет знать, что с этим делать.
	Решением этой проблемы является использование своего рода токенизации, в которой большинство токенов
	являются словами, но некоторые токены являются частыми морфемами или другими подсловами, такими как <<-ший>>, так что не появившееся в учебном корпусе слово может быть представлено путем объединения частей.
	
	Самый простой такой алгоритм - это кодирование байтовой пары, или BPE (Byte Pair Encoging, также известный как Digram Coding [5]).
	
	Алгоритм начинается с набора символов, равного набору букв в алфавите. Каждое	слово представлено в виде последовательности символов плюс специальный символ конца слова. На каждом шаге алгоритма мы подсчитываем количество пар букв, находим
	наиболее часто встречающуюся пару (A, B) и заменяем ее новым символом (AB). Мы
	продолжаем считать и объединять, пока
	не сделали k слияний; k является параметром алгоритма. Результирующий набор символов
	будет состоять из исходного набора букв плюс k новых символов.
	Конечно, в реальных алгоритмах BPE выполняется со многими тысячами слияний на очень больших входных словарях. В результате большинство слов будет представлено как один символ и только очень редкие слова (и неизвестные слова) должны быть представлены по частям.
	
	\begin{center}
		\includegraphics[scale=0.8]{bpe.png}
		\\ Рисунок 2 -- пример выполнения алгоритма \\ BPE c 7 операциями слияния
	\end{center}
	
	
	\subsection{WordPiece}
	
	Есть несколько альтернатив алгоритму BPE. Как и BPE, алгоритм WordPiece начинается с некоторой простой токенизации (например, по пробелам), а затем разбивает получившиеся грубые лексемы на токены подслов. Модель WordPiece отличается от BPE тем, что специальные маркеры границ слова появляется в начале слова, а не в конце, и в том, как он объединяет пары. Вместо того, чтобы объединять наиболее часто встречающиеся пары, WordPiece объединяет те пары, которые максимизируют схожесть текста с выбранной языковой моделью на обучающей выборке. Тогда каждое слово токенизируется с использованием жадного алгоритма с самым длинным соответствием префиксу. Это отличается от алгоритма декодирования, который был введен для BPE, выполняющий слияния
	на тестовом тексте в том же порядке они были извлечены из учебного набора.
	Жадное декодирование с самым длинным соответствием префиксу иногда называют максимальным соответствием или максимальным паросочетанием.
	
	\subsection{Нормализация слов, лемматизация, стемминг}
	Нормализация слов - это задание слов / токенов в стандартном формате, выбор одной нормальной формы для слов с несколькими формами.
	
	\textit{Сжатие регистра} - переводит все слова в тексте в нижний регистр, это очень полезно для обобщения во многих задачах, таких как поиск информации или распознавание речь. Для анализа тональности текста и других задач классификации текста, извлечения информации и машинного перевода, напротив, регистр может оказаться весьма полезными и
	сжатие вообще не применяется.
	
	\textit{Лемматизация} определяет, имеют ли два слова имеют одинаковый корень,	несмотря на их поверхностные различия. Например, слова \textit{am}, \textit{are} и \textit{is} имеют общую лемму \textit{be}; 
	Самые сложные методы лемматизации предполагают полный морфологический анализ слова. (Морфология - наука о том, как слова строятся из более мелких значащих единиц, называемых морфемами)
	
	
	Алгоритмы лемматизации могут быть достаточно сложными. По этой причине иногда используются более простые, но грубые методы, который в основном заключаются в обрезке аффиксов окончаний слова. Этот наивный вариант морфологического анализа называется \textit{стемминг}. Одним из наиболее распространенных алгоритмов стемминга является стеммирование Портера (1980). [6]\\
	
	Стеммер Портера, примененный к следующему параграфу:
	
	\vspace{5mm}
	\begingroup
	\leftskip4em
	\rightskip\leftskip\textit{	This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of the red crosses and the written notes.}
	\vspace{5mm}
	\endgroup
	
	производит такой стеммированный вывод:
	
	\vspace{5mm}
	\begingroup
	\leftskip4em
	\rightskip\leftskip\textit{Thi wa not the map we found in Billi Bone s chest but an accur copi complet in all thing name and height and sound with the singl except of the red cross and the written note}
	\vspace{5mm}
	\endgroup
	
	
	\subsection{Расстояние Левенштейна}
	
	Расстояние Левенштейна [7] или Minimum edit distance между двумя строками определяется как минимальное количество операций редактирования (такие операции, как: вставка, удаление,
	замена) необходимых для того, чтобы преобразовать одну строку в другую.
	Разрыв между \textit{intention} и \textit{execution}, например, равен 5 (удалить i, заменить e на n, заменить x на t, вставить c, заменить u на n). Намного легче увидеть это, посмотрев на Рисунок 3.
	
	
	\begin{center}
		\includegraphics[scale=0.8]{MED.PNG}
		\\ Рисунок 3 -- Расстояние Левенштейна между словами "intention" и "execution". \\ Описание операций: d - удаление, s - замена, i - вставка.
	\end{center}
	
	\section{Языковые модели основанные на N-граммах.}
	Модели, которые сопоставляют последовательностям слов вероятности называются \textit{Language Models} (\textit{LM}, \textit{Языковые Модели}).
	
	\textit{N-грамма} - это последовательность из N слов. 2-грамму называют \textit{биграммой} (\textit{bigram}), 3-грамму называют \textit{триграммой} (\textit{trigram}).
	В области обработки естественного языка N-граммы используется в основном для предугадывания на основе вероятностных моделей. N-граммная модель рассчитывает вероятность последнего слова N-граммы, если известны все предыдущие. При использовании этого подхода для моделирования языка предполагается, что появление каждого слова зависит только от предыдущих слов. [8]
	
	Другим применением N-грамм является выявление плагиата. Если разделить текст на несколько небольших фрагментов, представленных N-граммами, их легко сравнить друг с другом и таким образом получить степень сходства анализируемых документов[9]. N-граммы часто успешно используются для категоризации текста и языка. Кроме того, их можно использовать для создания функций, которые позволяют получать знания из текстовых данных. Используя N-граммы, можно эффективно найти кандидатов, чтобы заменить слова с ошибками правописания.
	
	\subsection{N-граммы}
	Рассмотрим задачу вычисления \textit{P(w|h)} - вероятности, что следующим словом является \textit{w}, при данной истории \textit{h}. Пусть история h: \textit{its water is so transparent that}, нужно узнать вероятность того, что следующим словом является \textit{the}, то есть, нужно найти:
	
	\begin{equation}
		\textit{P(the | its water is so transparent that)}
	\end{equation}
	
	Один из способов оценить эту вероятность - по относительным частотам:	можно взять очень большой корпус, подсчитать, сколько раз встречается \textit{its water is so transparent that}, подсчитать, сколько раз за этим последует \textit{the}. Тогда это было бы ответом на вопрос "Мы видели историю h некоторое число раз, сколько раз следующее слово было \textit{the}":
	
	\begin{equation}
		\textit{P(the | its water is so transparent that) } = \dfrac{\textit{C(its water is so transparent that the)}}{\textit{C(its water is so transparent that)}}
	\end{equation}
	
	С огромным корпусом (например, Глобальная Сеть) можно рассчитать и посчитать вероятности с помощью этой формулы.
	
	Метод расчета вероятностей таким методом работает в многих случаях, но оказывается, что даже Глобальная Сеть недостаточно велика, чтобы дать хорошие оценки в большинстве случаев. Это происходит потому, что новые предложения создаются все время.
	
	Похожим образом, если нужно узнать вероятность появления какой-то последовательности из \textit{n} слов, нужно посчитать, сколько раз она появляется вреди всем последовательностей из \textit{n} слов. На это требуется огромная вычислительная сила.
	
	Есть способ умнее:
	обозначим вероятность того, что конкретное слово \textit{X\textsubscript{i}} примет значение \textit{w\textsubscript{i}} или P(\textit{X\textsubscript{i}} = \textit{w\textsubscript{i}}) за P(\textit{w\textsubscript{i}}). Обозначим последовательность N слов как w\textsubscript{1}, w\textsubscript{2}, \dots{}, w\textsubscript{n} и как w\textsubscript{1}\textsuperscript{n}. Обозначим совместную вероятность того, что каждое слова в последовательности имеет какое-то значение \textit{P(X = w\textsubscript{1}, Y = w\textsubscript{2}, \dots{}, W = w\textsubscript{n})} за \textit{P(w\textsubscript{1}, w\textsubscript{2}, \dots{}, w\textsubscript{n})}. Теперь, чтобы вычислить вероятность целой последовательности \textit{P(w\textsubscript{1}, w\textsubscript{2}, \dots{}, w\textsubscript{n})} можно воспользоваться цепным правилом вероятностей: 
	
	\begin{equation}
	P(X\textsubscript{1}, \dots{}, X\textsubscript{n}) = P(X\textsubscript{1}) P(X\textsubscript{2} | X\textsubscript{1}) P(X\textsubscript{3} | X\textsubscript{1}\textsuperscript{2}) \dots{} P(X\textsubscript{n} | X\textsubscript{1}\textsuperscript{n-1})
	\end{equation}
	 
	 Цепное правило показывает взаимосвязь между вычислением совместной вероятности последовательности и вычислением условной вероятности слова, при данных предыдущих словах. Предыдущая формула предполагает, что можно оценить совместную вероятность целой последовательности слов умножая между собой условные вероятности. Но похоже, что цепное правило не сильно-то и помогает, так как неизвестен способ вычисления точной вероятности слова после длинной последовательности предыдущих слов. P(X\textsubscript{n} | X\textsubscript{1}\textsuperscript{n-1}). Но с помощью N-грам можно аппроксимировать историю по нескольким предыдущим словам. 
	
	Биграмма, например, аппроксимирует вероятность слова по 1 предыдущему слову, то есть
	P(w\textsubscript{n} | w\textsubscript{1}\textsuperscript{n-1}) $\approx$ P(w\textsubscript{n} | w\textsubscript{n-1}). 
	
	Предположение, что вероятность слова зависит только от предыдущего слова называется Марковым предположение. Марковские модели - это класс вероятностных моделей, чтобы предполагают, что можно предсказать вероятность какого-то будущего события не смотря слишком далеко назад. Можно обобщить биграмму (которая смотрит только на одно слово в прошлое) до триграммы (которая смотрит на 2 слова в прошлое), а значит, по индукции, до N-граммы (которая смотрит на n-1 слово в прошлое).
	
	Таким образом, общее уравнение для этой N-граммной аппроксимации условной вероятности следующего слова в предложении таково:
	
	\begin{equation}
	 P(w_n | w_1^{n-1}) \approx P(w\textsubscript{n} | w\textsubscript{n+1-N}\textsuperscript{n-1}) 
	\end{equation}
	 
	 При заданном биграмном предположении для вероятности отдельного слова, можно вычислить вероятность полной последовательности слов:
	 
	\begin{equation}
		P(w\textsubscript{1}\textsuperscript{n}) \approx \prod_{k=1}^{n} P(w_k | w_{k-1})
	\end{equation}
	
	Для того, чтобы оценить вероятности этих биграм или n-грам можно использовать \textit{метод наибольшего правдоподобия} (\textit{maximum likelihood estimation}, \textit{MLE}). 
	
	Мы получаем оценки MLE для параметров N-граммы подсчитав их в корпусе и нормализовав их таким образом, чтобы они лежали между 0 и 1.
	
	Например, чтобы вычислить конкретную биграммную вероятность слова \textit{y} при предыдущем слова \textit{x}, нужно вычислить количество биграмм \textit{C(xy)} и нормализовать по сумма всех биграмм, у которых первое слово \textit{x}.
	
	\begin{equation}
		P(w_n | w_{n-1}) = \dfrac{C(w_{n-1}w_n)}{\sum_w C(w_{n-1}w)}
	\end{equation}
	
	Эту формулу можно упростить, заметив, что количество биграмм, начинающихся с $w_{n-1}$ должно быть равно количество слов $w_{n-1}$. 
	
	\begin{equation}
		P(w_n | w_{n-1}) = \dfrac{C(w_{n-1}w_n)}{C(w_{n-1})}
	\end{equation}


	Для общего случая MLE, оценка параметра n-граммы:
	
	
	\begin{equation}
		P(w_n | w_{n-N+1}^{n-1}) = \dfrac{C(w_{n-N+1}^{n-1} w_n)}{C(w_{n-N+1}^{n-1})}
	\end{equation}
	
	Эта формула оценивает вероятность N-граммы деля увиденную частоту конкретной последовательности на увиденную частоту префикса. Это отношение называется \textit{относительной частотой}.
	
	В практике применяются триграммы, 4-граммы или даже 5-граммы, в зависимости от объема обучающей выборки. 
	
	Также, чаще всего используются \textit{логарифмические вероятности}, так как перемножение большого числа вероятностей может дать слишком маленькое число, которое может "сломать" разрядную сетку вещественного числа в цифровом представлении. При использовании логарифмов умножение заменяется на сложение, следовательно, число не будет слишком мало.
	
	
	\begin{equation}
		p_1 \times p_2 \times p_3 \times p_4 = exp(log(p_1) + log(p_2) + log(p_3) + log(p_4))
	\end{equation}
	
	\subsection{Оценка языковых моделей}
	Лучший способ оценить производительность языковой модели - это встроить её в приложение и вычислить, насколько приложение улучшается. Такой подход называется \textit{внешней оценкой}. Внешняя оценка - это единственный способ узнать, помогает ли в решении задачи конкретное улучшение какой-то компоненты. Таким образом, для распознавания речи можно сравнить производительность двух языковых моделей, запустив дважды распознаватель и посмотреть, какой запуск дает более точный перевод.
	
	К сожалению, запуск больших NLP систем от начала до конца - это очень дорого. Вместо этого хорошо бы иметь метрику, которая может дать быструю оценку потенциального улучшения в языковой модели. Метрика \textit{внутренней оценки} - это одна из мер качества модели, вне зависимости от приложения. 
	
	Для внутренней оценки языковой модели требуется \textit{тестовая выборка}. Как и с многими другими статистическими моделями, вероятности N-граммовой модели выходят из корпуса, на котором модель обучалась, которая называется \textit{обучающей выборкой}. После этого можно оценить качество модели на основе её производительности на каких-то еще неиспользованных данных, которые и называются \textit{тестовой выборкой}.
	
	Если задан какой-то корпус текста, он делится на обучающую и тестовую выборки, после этого обучаются на обучающей и проверяются на тестовой. Потом происходит сравнение двух обученных моделей: насколько хорошо они удовлетворяют тестовой выборке.
	
	Очень важно не допустить того, чтобы информация из тестовой выборки встречалась в обучающей. Если это происходит, попавшему туда предложению ошибочно ставится большая вероятность. Это называют \textit{обучением на тестовой выборке}. 
	
	Иногда тестовая выборка используется настолько часто, что модель явным образом подгоняется под её характеристики. Для того, чтобы этого избежать, используют свежие данные, не использованные ни в тестовой, ни в обучающей выборке. Эти данные называют \textit{валидационной выборкой} (\textit{development test set}, \textit{devset}). 
	
	\subsection{Перплексия}
	На практике, вероятности не используются для метрики оценки языковой модели. Вместо этого используется \textit{перплексия} (иногда называют \textit{PP}). Перплексия языковой модели на тестовой выборке - это обратная величина к вероятности на тестовой выборке, нормализованная количкством слов. Для тестовой выборки $W = w_1 w_2 \dots{} w_N$ :
	
	\begin{equation}
		PP(W) = P(w_1 w_2 \dots{} w_n)^{-\frac{1}{N}}
	\end{equation}
	
	Можно использовать цепное правило, чтобы раскрыть вероятность W:
	
	\begin{equation}
		PP(W) = (\prod_{i=1}^{N} \frac{1}{P(w_i | w_1 \dots w_{i-1})})^{\frac{-1}{N}}
	\end{equation}
	
	Если вычислять перплексию с помощью биграммной языковой модели:
	
	
	\begin{equation}
		PP(W) = (\prod_{i=1}^{N} \frac{1}{P(w_i | w_{i-1})})^{\frac{-1}{N}}
	\end{equation}
	
	Можно представить перплекцию по-другому, \textit{как взвешенное среднее количество сыновей вершины графа}. Количество сыновей - это количество различных слов, которые могут последовать за текущим.
	
	\subsection{Неизвестные слова}
	Неизвестные слова не могут появиться в тестовой выборке, если используется \textit{закрытый словарь}, который содержит все слова в лексиконе. В других случаях придется обрабатывать слова, никогда ранее не встречавшиеся, назовем такие слова \textit{неизвестными} или \textit{словами вне словаря} (\textit{out of vocabulary}, \textit{OOV}). Процент неизвестных слов, которые попадаются в тестовой выборке, называют \textit{мерой неизвестности} (\textit{OOV rate}). Система с \textit{открытым словарем} - в которой неизвестные слова добавляются как псевдо-слово <UNK>.
	
	Есть несколько способов обучить вероятности слов <UNK>. Один из них - это свести задачу назад к закрытому словарю, добавив слово <UNK> в словарь на стадии нормализации текста, после этого обращаясь к нему как к любому другому слову.
	
	\subsection{Сглаживание}
	Самый простой способ - это просто добавить один по всем счетам биграмм перед тем, как производить нормализацию их в вероятности. Этот алгоритм называется \textit{сглаживанием Лапласа}. Оно недостаточно хорошо себя показывает, чтобы использовать его в современных моделях на N-граммах, но дает множество полезных идей, которые используются в других алгоритмах сжатия, также является практичным алгоритмам для таких задач, как \textit{классификация текста}.
	
	Оценка вероятности появления слова $w_i$ - это его количество $с_i$, нормализованное общим числом слов $N$.
	
	\begin{equation}
		P(w_i) = \dfrac{c_i}{N}
	\end{equation}
	
	Сглаживание Лапласа добавляет 1 ко всем счетам. Всего есть $V$ уникальных слов в словаре, к каждому добавляется 1, значит знаменатель увеличивается на $V$.
	
	\begin{equation}
		P_{Laplace}(w_i) = \dfrac{c_i + 1}{N + V}
	\end{equation}
	
	
	Сглаживание для вероятностей биграмм:
	
	\begin{equation}
		P^{*}_{Laplace}(w_n | w_{n-1}) = \dfrac{C(w_{n-1} w_n) + 1}{\sum_w (C(w_{n-1} w) + 1)} = \dfrac{C(w_{n-1} w_n) + 1}{C(w_{n-1}) + V}
	\end{equation}
	
	Альтернативный способ сглаживания - прибавлять ко всем счетам не 1, а k. Этот алгоритм называется \textit{<<прибавь k>>-сглаживание} (\textit{add-k smoothing}).
	
	\begin{equation}
		P^{*}_{Add-k}(w_n | w_{n-1}) = \dfrac{C(w_{n-1} w_n) + k}{\sum_w (C(w_{n-1} w) + k)} = \dfrac{C(w_{n-1} w_n) + 1}{C(w_{n-1}) + kV}
	\end{equation}
	 
	 \subsection{Интерполяция}
	 
	 Иногда, когда требуется вычислить $P(w_n | w_{n-1})$, но нет примера триграммы $w_{n-2}w_{n-1}w_{n}$, можно оценить вероятность используя биграммную вероятность $P(w_{n} | w_{n-1})$. Похожим образом, если нет примеров биграммы $P(w_n | w_{n-1})$, можно воспользоваться $P(w_n)$.
	 
	 Примером такого использования является \textit{интерполяция}, она вычисляет оценку вероятности взвешивая и комбинируя триграмму, биграмму и юниграмму.
	 
	 В простой линейной интерполяции оценки комбинируются линейно:
	 
	 \begin{equation}
	 	P^{'}(w_n | w_{n-2} w_{n-1}) = \lambda_1 P(w_n | w_{n-2} w_{n-1}) + \lambda_2 P(w_n | w_{n-2}) + \lambda_3 P(w_n)
	 \end{equation}
	 
	 
	 \begin{equation}
	 	\sum_i \lambda_i = 1
	 \end{equation}
	 
	 Способ сложнее - давать каждой $\lambda$ вес, зависящий от контекста. Если имеются точные счеты для какой-то биграммы, можно сделать предположение, что счеты для триграмм, основанные на этой биграмме будут заслуживать большего доверия, а значит можно дать соответствующей $\lambda$ больший вес:
	 
	 \begin{equation}
	 	P^{'}(w_n | w_{n-2} w_{n-1}) = \lambda_1 (w_{n-2}^{n-1}) P(w_n | w_{n-2} w_{n-1}) + \lambda_2 (w_{n-2}^{n-1}) P(w_n | w_{n-2}) + \lambda_3 (w_{n-2}^{n-1}) P(w_n)
	 \end{equation}
	 
	  Для того, чтобы вычислить все $\lambda_i$, пользуются \textit{сохранённым} (\textit{held-out}) корпусом - дополнительным обучающей выборкой, которую используют для настройки параметров системы, таких, как $\lambda$. 
	
	\section{Подходы и методы, основанные на наивном Байесе.}
	
	\textit{Наивный Байес} зачастую применяется к задаче \textit{категоризации текста} - присваиванию категории к целым текстам или документам, например, к \textit{анализу тональности текста}, то есть выделению, например, положительного или отрицательного настроя автора к какому-либо объекту.
	
	 \textit{Обнаружение спама} - еще одно приложение. Задача бинарной классификации, состоящая в том, чтобы дать электронному письму оценку, является ли оно спамом.
	 
	 Цель классификации состоит в том, чтобы произвести простые наблюдения, выделить какие-либо полезные черты и классифицировать наблюдения в какой-то дискретный класс.
	 
	 \subsection{Наивный Байес}
	 
	 Интуитивное описание показано на рисунке 4. Текст представляется как \textit{мешок слов}, то есть неупорядоченное множество слов, их взамное расположение проигнорировано, сохранены только частоты появления слова в тексте.
	 
	 
	 \begin{center}
	 	\includegraphics[scale=0.9]{NaiveBayes.PNG}
	 	\\ Рисунок 4 - Интуитивное описание классификатора на наивном Байесе, применённое к обзору фильма. Взаимное расположение слов игнорировано (взято предположение \textit{мешка слов}).
	 \end{center}
	  
	  Наивный Байес - это вероятностный классификатор, означающий, что для документа $d$ из всех классов $c \in C$ классификатор возвращает класс $\hat{c}$ который имеет максимальную вероятность встречи в тексте. В следующей формуле нотация \^ означает предполагаемый корректный класс. 
	
	\begin{equation}
		\hat{c} = \underset{c \in C}{argmax} P(c | d)
	\end{equation}
	
	Эта идея \textit{Байесовского вывода} была известна с момента публикации работы Байеса (1763)[10], и была впервые применена к классификации текста Мостеллером и Уоллесом (1964)[11]. Идея Байесовского классификатора - использовать правило Байеса и трансформировать предыдущее уравнение в другие вероятности, имеющие полезные свойства. Правило Байеса представлено в следующем уравнении. Оно дает способ разбить условную вероятность $P(x | y)$ в три другие вероятности:
	
	
	\begin{equation}
		P(x | y) = \dfrac{P(y | x) P(x)}{P(y)}
	\end{equation}
	
	Используя предыдущую формулу.
	
	\begin{equation}
		\hat{c} = \underset{c \in C}{argmax} P(c | d) = \underset{c \in C}{argmax} \dfrac{P(d | c) P(c)}{P(d)}
	\end{equation}
	
	Знаменатель можно опустить.
	
	\begin{equation}
		\hat{c} = \underset{c \in C}{argmax} P(c | d) = \underset{c \in C}{argmax} P(d | c) P(c)
	\end{equation}

	Таким образом вычисляется самый вероятный класс $\hat{c}$ по заданному документу $d$ с помощью класса который имеет наибольшее произведение двух вероятностей: априорная вероятность класса $P(c)$ и функция правдоподобия документа  $P(d | c)$.
	
	
	\begin{equation}
		\hat{c} = \underset{c \in C}{argmax} P(c | d) = \underset{c \in C}{argmax} \overset{likelihood}{P(d | c)} \overset{prior}{P(c)}
	\end{equation}
	
	Не умаляя общности, можно представить документ $d$ как набор черт $f_1, f_2, \dots, f_n$.
	
	
	\begin{equation}
		\hat{c} = \underset{c \in C}{argmax} \overset{likelihood}{P(f_1, f_2, \dots, f_n | c)} \overset{prior}{P(c)}
	\end{equation}
	
	
	К сожалению, даже эта формула тяжела для вычисления: без какиих-либо упрощающих предположений оценка вероятность любой возможной комбинации черт потребует огромного числа параметров и невозможно большую обучающую выборку. Поэтому Наивный Байес делает два упрощающих предположения.
	
	Первое - это \textit{мешок слов} (\textit{bag of words}): предположение, что позиция слова не имеет значения. Например, слово \textit{любовь} будет иметь один и тот же эффект вне зависимости от того, чтоит оно на первом, двадцатом или последнем месте в тексте.
	
	Второе часто называют \textit{предположением наивного Байеса}:
	это предположение условной независимости того, что вероятности $P(f_i | c)$ независимы при данном классе $c$ и могут быть <<наивно>> перемножены таким способом:
	
	\begin{equation}
		P(f_i, f_2, \dots f_n | c) = P(f_1 | c) P(f_2 | c) \dots P(f_n | c)
	\end{equation}
	
	Результирующее уравнение для класса выбранного наивным Байесом таково:
	
	\begin{equation}
		C_{NB} = \underset{c \in C}{argmax} P(c) \underset{f \in F}{\prod} P(f | c)
	\end{equation}
	
	Для того, чтобы применить наивного Байеса к тексту, нужно учесть позиции, просто проверяя индексом каждую позицию слова в документе:
	
	\begin{center}
	positions $\leftarrow$ все позиции слов в тестовом документе
	\end{center}
	\begin{equation}	
	C_{NB} = \underset{c \in C}{argmax} P(c) \underset{i \in positions}{\prod} P(w_i | c)
	\end{equation}
	
	 
	 \subsection{Наивный Байес как лингвистическая модель}
	 
	 Байесовские классификаторы могут использовать любые виды черт: словари, URL, e-mail адреса, черты сети, фразы и тому подобное. Но если, как в предыдущей секции, используются только черты отдельных слов и используются все слова в тексте, то наивный Байес имеет важную схожесть с моделированием речи. Конкретнее, модели на наивном Байесе могут быть рассмотрены как множество классо-специфичных юниграммных лингвистических моделей.
	 
	 Поскольку  правдоподобие черт из модели наивного Байеса присваивает вероятность для каждого слова $P(word | c)$, модель также присваивает вероятность целым предложениям.
	 
	 \begin{equation}
	 	P(s | c) = \prod_{i \in positions} P(w_i | c) 
	 \end{equation}
	 
	 \subsection{Метрики: точность, полнота, F-мера}
	 
	 Рассмотрим задачу обнаружения спама. Задача состоит в том, чтобы отнести каждое электронное письмо к одной из двух категорий: положительной (то есть, является спамом) или отрицательной (то есть, не является). Таким образом, для каждого письма требуется узнать, считает ли система его спамом или нет. Также требуется знать, является ли письмо спамом на самом деле, то есть определенные человеком ярлыки для каждого письма.Таким метки будут далее называться \textit{золотыми ярлыками} (\textit{gold labels}).
	 
	 Для оценки любой системы распознавания требуется начать с постройки \textit{таблицы сопряженности}.
	 
	 \begin{center}
	 	\includegraphics[scale=0.9]{conttable.PNG}
	 	\\Рисунок 5 - таблица сопряженности
	 \end{center}
	 
	 В правом нижнем углу таблицы уравнение для \textit{accuracy}, которое показывает какую часть всех наблюдений система обозначила правильно. Хотя accuracy может показаться вполне естественной метрикой, она используется нечасто. Причина заключается в том, что accuracy работает плохо, если классы не сбалансированы. Другими словами, accuracy плоха, если требуется найти что-то редко встречающееся, или по крайней мере просто не сбалансировано по частоте встречи. 
	 
	 Именно поэтому вместо accuracy используются 2 метрики: \textit{полнота} и \textit{точность} (\textit{precision} и \textit{recall}).
	 
	 \textit{Точность} обозначает долю прецедентов которые система система обозначила как положительные и которые на самом деле являются положительными (то есть, например, обозначила как спам и письмо действительно им является).
	 
	 \begin{equation}
    	Precision = \dfrac{\textit{true positives}}{\textit{true positioves + false positives}}
	 \end{equation}
	 
	 \textit{Полнота} обозначает долю прецедентов, которые были корректно обозначены положительными (то есть среди всех писем обозначенных как спам доля тех, которые действительно им являлись).
	 
	 \begin{equation}
	 Precision = \dfrac{\textit{true positives}}{\textit{true positioves + false negatives}}
	 \end{equation}
	 
	 Есть много способов определения единой метрики, которая сочетает в себе оба аспекта полноты и точности. Самая простая из этих комбинаций: \textit{F-мера}.
	 
	 \begin{equation}
	 F_\beta = \dfrac{(\beta^2 + 1) P R}{\beta^2 P + R} 
	 \end{equation}
	 
	 Параметр $\beta$ взвешивает важность полноты и точности, его задают в зависимости от задачи. Значения $\beta$ > 1 отдают предпочтение точности, $\beta$ < 1 - полноте. Когда $\beta$ = 1, метрики равно сбалансированы; это самый частый случай использования, обозначается $F_{\beta = 1}$ или $F_1$: 
	 
	 \begin{equation}
	 F_1 = \dfrac{2PR}{P + R}
	 \end{equation}
	 
	 
	 Для случая, когда есть больше, чем 2 класса.
	 В обработке языка часто встречается \textit{мультиномиальная классификация}, в который классы - взаимоисключающие, каждый документ относится к одному классу. Для каждого класса строится свой бинарный классификатор, обучающийся на положительных результатах из класса \textit{c} и отрицательных из всех остальных классов. После этого заданный документ \textit{d} обрабатывается всеми классификаторами и выбирается ярлык из классификатора с наивысшим результатом. Рассмотрим \textit{матрицу ошибок} для гипотетической категоризации писем по классам \textit{важные}, \textit{обычные}, \textit{спам} (\textit{urgent}, \textit{normal}, \textit{spam}).
	 
	 \begin{center}
	 	\includegraphics[scale=0.9]{confmatrix.PNG}
	 	\\Рисунок 6 - матрица ошибок (confusion matrix)
	 \end{center}
	
	Для того, чтобы выделить единую метрику для производительности системы в целом, можно воспользоваться \textit{макроусреднением} (\textit{macroaveraging}) или \textit{микроусреднением} (\textit{microaveraging}).
	
	\textit{Макроусреднение} - это вычисление производительности для каждого класса, а потом взятие среднего.
	\textit{Микроусреднение} - это сбор всех решений в единую таблицу сопряженности и вычисление полноты и точности по ней. На следующем рисунке показана разница между этими двумя способами. 
	
	\begin{center}
		\includegraphics[scale=0.9]{micromacro.PNG}
		\\Рисунок 7 - макроусреднение и микроусреднение
	\end{center}
	
	
	\subsection{Тестовые выборки и кросс-валидация}
	С фиксированными выборками возникает следующая проблема: так как для обучающей выборки нужно как можно больше прецедентов, тестовая или валидационная выборка могут оказаться недостаточно большими, чтобы быть репрезентативными. Лучшим вариантом было бы, если можно было бы использовать всю информацию а одновременно тестовой и обучающей выборке. Это позволяет сделать \textit{кросс-валидация}:
	случайным образом весь датасет разбивается на обучающую и тестовую выборки, классификатор обучается и проверяется на разбитых множествах. 
	Эта процедура происходит 10 раз, средний результат среди 10 попыток дает оценку ошибок. Всё это называется \textit{10-слойной кросс-валидацией}.
		
		
	\section{Логистическая регрессия}	
	\textit{Логистическая регрессия} - один из самых важных аналитических инструментов в социальных и естественных науках. В обработке естественного языка логистическая регрессия - базовый метод машинного обучения с учителем для классификации. Имеет очень близкое отношение к нейронным сетям. 
	Логистическая регрессия используется, чтобы классифицировать наблюдения в одни из 2 или более классов. Так как математика под двухклассовым случаем проще, сначала будет описана именно она. Затем будет описан случай мультиномиальной логистической регрессии для случая большего числа классов. 
	
	\subsection{Генеративные и дискриминативные классификаторы}.
	
	Самое важное отличие наивного Байеса от логистической регрессии состоит в том, что логистическая регрессия - \textit{дискриминативный классификатор}, а наивный Байес - \textit{генеративный}. Можно рассмотреть на визуальной метафоре: пусть задачей является отделить картинки кошек от собак. 
	
	\textit{Генеративный} классификатор будет иметь цель понять как выглядит кошка и собака. Можно даже заставить модель <<сгенерировать>>, то есть нарисовать собаку. При подаче тестового изображения система спрашивает, какая модель - кошки или собаки лучше подходит под изображение и, отталкиваясь от этого, выбирает ярлык.
	
	\textit{Дискриминативная} модель учится именно разделять классы. Возможно, все собаки носят ошейники, а все кошки - не носят. Если эта черта почти полностью разделяет классы, модели этого достаточно. Если спросить модель, что она знает о кошках, она ответит, что они не носят ошейников.
	
	Генеративная модель, такая, как наивный Байес, использует термин \textit{правдоподобие}, который выражает, как генерировать черты документа, если бы мы знали какого он класса.
	
	Дискриминативная модель пытается непосредственно вычислить $P(c | d)$. Похоже, что она учится присваивать веса к чертам документа, которые непосредственно улучшают способность разделять между классами, даже если модель не может сгенерировать пример одного из классов.
	
	
	\subsection{Компоненты вероятностного классификатора для машинного обучения}
	Как и наивный Байес, логистическая регрессия - это вероятностный классификатор, использующий \textit{обучение с учителем}. Классификаторы машинного обучения требуют корпус из M пар ввода-вывода ($x^{(i)}$, $y^{(i)}$).
	
	Система машинного обучения для классификации имеет 4 компоненты:
	\begin{enumerate}
		\item Репрезентация черт ввода. Для каждого наблюдения $x^{(i)}$ это вектор черт $[x_1, x_2, \dots, x_n]$. Обозначение: черта \textit{i} прецедента $x^{(j)}$ это $x_i^{(j)}$, иногда упрощенно $x_i$, иногда $f_i$, $f_i(x)$, или, для мультиклассовой классификации, $f_i(c, x)$.
		
		\item Функция классификации, которая вычисляет $\hat{y}$, предполагаемый класс, как $p(y | x)$. В следующей секции будут введены инструменты \textit{сигмоида} и \textit{софтмакс} для классификации. 
		
		\item Целевая функция для обучения, обычно включающая минимизацию ошибки на обучающей выборке. 
		
		\item Алгоритм для оптимизации целевой функции. Далее в работе будет представлен \textit{алгоритм стохастического градиентного спуска}.		
		
	\end{enumerate}
	
	
	Логистическая регрессия имеет 2 фазы:
	\begin{enumerate}
		\item обучение: система учится (а именно веса $w$ и $b$), используя стохастический градиентный спуск и функцию потерь \textit{кросс-энтропии}.
		
		\item проверка: по заданному прецеденту $x$ вычисляется $P(y | x)$ и возвращается большая вероятность: $y = 1$ или $y = 0$.
		
	\end{enumerate}
	
	
	\subsection{Классификация: сигмоида}
	
	Целью бинарной логистической регрессии является обучение классификатора таким образом, чтобы он мог давать бинарное предсказание о классе нового прецедента. Здесь будет описана \textit{сигмоида}, помогающая принять такое решение.
	
	Рассмотрим единственный прецедент $x$, который будет обозначаться как вектор черт $[x_1, x_2, \dots, x_n]$. Вывод классификатора может $y$ может быть равен 1 (это означает, что прецедент является членом класса) или 0 (не является). Требуется узнать вероятность $P(y = 1 | x)$ того, что прецедент является членом класса.
	
	Логистическая регрессия добивается этого, обучая на выборке вектор весов и меру смещения. Каждый вес $w_i$ - действительное число, ассоциировано с чертой $x_i$. Вес обозначает, насколько важна эта черта для классификации определенного класса, вес может быть положительным (значит, что ассоциировано) или отрицательным (не связано). 
	
	Чтобы сделать решение на тестовом случае, после того, как веса обучены, классификатор умножает каждый $x_i$ на его вес $w_i$ и прибавляет меру смещения $b$. Результирующее число $z$ обозначает взвешенную сумму всех всидетельств класса.
	
	\begin{equation}
	z = (\sum_{i=1}^n w_i x_i) + b
	\end{equation}
	
	Можно записать это уравнение короче, если воспользоваться скалярным произведением:
	
	\begin{equation}
	z = w \cdot x + b
	\end{equation}
	
	
	В предыдущем уравнении ничего не останавливает $z$ от того, чтобы перестать быть валидной вероятностью, то есть выйти из границ $[0, 1]$.  
	
	\begin{center}
		\includegraphics[scale=0.8]{sigmoid.PNG}
		\\ Рисунок 8 -- сигмоида, на вход получает действительное число, на выходе - число из интервала $[0, 1]$, при этом практически линейна вокруг 0
	\end{center}
	
	Для того, чтобы получить вероятность, $z$ пропускают через \textit{сигмоиду} $\sigma (z)$. Сигмоиду также называют логистической функцией.
	
	\begin{equation}
	y = \sigma (z) = \dfrac{1}{1 + e^{-z}}
	\end{equation}
	
	
	Сигмоида также дифференциируема, это очень полезное свойство. 
	
	Если применить сигмоида к сумме взвешанных черт, получится число в интервале $[0, 1]$. Для того, чтобы сделать её вероятностью, нужно просто сделать так, чтобы $p(y = 0) + p(y = 0) = 1$.
	
	\begin{equation}
	P(y = 1) = \sigma(w \cdot x + b) = \dfrac{1}{1 + e^{-(w \cdot x + b)}}
	\end{equation}
	
	\begin{equation}
	P(y = 0) = 1 - \sigma(w \cdot x + b) = \dfrac{e^{-(w \cdot x + b)}}{1 + e^{-(w \cdot x + b)}}
	\end{equation}
	 
	 Теперь, когда есть алгоритм, который по $x$ вычисляет вероятность $P(y = 1 | x)$, остается вопрос, как принять решение о классе. Можно, например, положить, что $x$ относится к классу, если $P(y = 1 | x)$ больше, чем $0.5$, и не относится в ином случае.
	 
	 \begin{equation}
	 \hat{y} = 
	 \begin{cases}
	 1 \textit{, если } P(y = 1 | x) > 0.5 \\
	 0, \textit{иначе}
	 \end{cases}
	 \end{equation}
	 
	 \subsection{Обучение логистической регрессии}
	 
	 Для того, чтобы обучить логистическую регрессию, требуется, 2 компоненты. 
	 
	 Первая - метрика того, насколько близка оценка $\hat{y}$ к действительному классу. Вместо того, чтобы оценивать похожесть, обычно говорят об обратном: расстоянии между выводом системы и действительным классом. Это расстояние обычно называют $функцией потерь$. В следующей главе будет представлена функция потерь, которая часто используется для логистической регрессии и нейронных сетей, $кросс-энтропия$. 
	 
	 Вторая компонента - это алгоритм оптимизации для итеративного обновления весов такого и минимизации функции потерь. Стандартный алгоритм для этого - \textit{градиентный спуск}. В следующей главе будет рассмотрен \textit{стохастический градиентный спуск}.
	
	\subsection{Функция потерь кросс-энтропии}
	
	Требуется функция потерь, которая выражает для прецедента $x$ как близко вывод классификатора ($\hat{y} = \sigma(w \cdot x + b)$) находится к корректному выводу ($y$, который равен $0$ или $1$). Назовем её так:
	
	\begin{equation}
	L(\hat{y}, y) = \textit{Насколько } \hat{y} \textit{ отличается от настоящего } y
	\end{equation}
	
	Это вычисляется с помощью функции потерь, которая считает, что корректный класс ярлыков более вероятен. Это называется \textit{условная оценка максимальной правдоподобности}: выбираются такие параметры $w$, $b$, что максимизируется логарифмическая вероятность настоящего $y$ в обучающей выборке. Результирующая функция потерь - \textit{кросс-энропическая}.
	
	Возьмем производную этой функции потерь, примененную к прецеденту $x$. Хотелось бы обучить веса таким образом, чтобы максимизировать вероятность $p(y | x)$. Поскольку есть только два дискретных выхода: 0 и 1, это распределение Бернулли, можно выразить вероятность $p(y | x)$, которую производит классификатор для одного прецедента как следующее уравнение:
	
	\begin{equation}
	P(y | x) = \hat{y}^y (1 - \hat{y})^{1 - y}
	\end{equation}
	
	Теперь возьмем логарифм от каждой части:
	
	\begin{equation}
	log p(y | x) - log[ \hat{y}^y (1 - \hat{y})^{1 - y}] = y log \hat{y} + (1 - y) log(1 - \hat{y})
	\end{equation}
	
	Предыдущее уравнение описывает логарифмическую вероятность, которая должна быть максимизирована.
	
	\begin{equation}
	L_{CE} (\hat{y}, y) = -log p(y | x) = -[y log \hat{y} + (1 - y) log(1 - \hat{y})]
	\end{equation}
	
	Подставляя $\hat{y} = (\sigma(w \cdot x + b))$:
	
	\begin{equation}
	L_{CE}(w, b) = -[y log \sigma(w \cdot x + b) + (1 - y) log(1 - \sigma(w \cdot x + b))]
	\end{equation}
	
	
	\subsection{Градиентный спуск}
	
	Цель градиентного спуска в том, чтобы найти оптимальные веса: минимизировать функцию потерь.
	
	\begin{equation}
	\hat{\theta} = \underset{\theta}{argmin} \dfrac{1}{m} \sum_{i=1}^{m} L_{CE} (y^{(i)}, x^{(i)}; \theta)
	\end{equation}
	
	Градиентный спуск - метод, который находит направление, в котором касательная функции растет быстрее всего и двигается в обратном направлении. Для логистической регрессии эта функция потерь выпукла.
	
	
	\begin{center}
		\includegraphics[scale=0.9]{gradstep.PNG}
		\\Рисунок 9 - шаг градиентного спуска, для одномерных векторов черт
	\end{center}
	
	По заданной случайной инициализации $w$ каким-то значением $w^1$ и предполагая, что функция потерь имеет форму, как на Рисунке 9, требуется алгоритм, который сообщает, что на следующей итерации нужно сдвинуться влево или вправо, чтобы достичь минимума.
	
	Алгоритм градиентного спуска отвечает на этот вопрос, находя градиент функции потерь в данной точке и двигаясь в направлении, противоположном найденному.
	
	Величина движения в градиентном спуске - это наклон касательной $\frac{d}{dx}f(x;w)$, взвешенный скоростью обучения $\eta$. Большая скорость обучения означает, что нужно двигать $w$ больше на каждой итерации. Изменение параметра - это скорость обучения, умноженная на градиент:
	
	\begin{equation}
	w^{t+1} = w^t - \eta \dfrac{d}{dx} f(x;w)
	\end{equation}
	
	\subsection{Градиент для логистической регрессии}
	
	Для того, чтобы обновить веса, нужно ввести определение градиента. Для функции потерь кросс-энтропии:
	
	\begin{equation}
	L_{CE}(w, b) = -[y log \sigma(w \cdot x + b) + (1 - y) log(1 - \sigma(w \cdot x + b))]
	\end{equation}
	
	Возьмем производную:
	
	
	\begin{equation}
	\dfrac{\partial L_{CE}(w,b)}{\partial w_j} = [\sigma (w \cdot x + b) - y] x_j
	\end{equation}
	
	\subsection{Алгоритм стохастического градиентного спуска}
	
	Стохастический градиентный спуск - это online-алгоритм минимизации функции потерь, который вычисляет градиент после каждого обучающего примера и корректирует $theta$ (множество всех весов).
	\\
	\noindent\fbox{%
		\parbox{\textwidth}{%
			\textbf{функция} Стохастический Градиентный Спуск ($L(), f(), x, y$) \textbf{возвращает} $\theta$ \\
			\text{ }\text{ } \# L - функция потерь \\
			\text{ }\text{ } \# f - функция, параметризированная $\theta$ \\
			\text{ }\text{ } \# x - вводы обучающей выборки \\
			\text{ }\text{ } \# y - выводы обучающей выборки (ярлыки) \\
			$\theta \leftarrow 0$ \\
			\textbf{Пока} требуется \\
			\text{ }\text{ } Для каждого прецедента ($x^{(i)}, y^{(i)}$) (в случайном порядке) \\
			\text{ }\text{ }\text{ }\text{ } 1. Вычислить $\hat{y}^{(i)}$ = $f(x^{(i)}; \theta)$ \\
			\text{ }\text{ }\text{ }\text{ } 2. Вычислить потерю $L(\hat{y}^{(i)}, y^{(i)})$ \\
			\text{ }\text{ }\text{ }\text{ } 3. $g \leftarrow \nabla_\theta L(f(x^{(i)}; \theta), y^{(i)})$ \\
			\text{ }\text{ }\text{ }\text{ } 4. $\theta \leftarrow \theta - \eta g$ \\
		}%
	}


	\begin{center}
		Рисунок 10 - алгоритм стохастического градиентного спуска. Алгоритм может завершиться, когда сойдется, то есть, например, веса практически не изменяются.
	\end{center}
	
	Параметр $\eta$ может быть скорректирован. Если он достаточно велик, то обучаемый будет делать слишком большие шаги, <<перепрыгивая>> минимум функции потерь, если слишком мал, то обучение будет занимаьт слишком много времени. Очень часто скорость обучения делают большой, затем постепенно её уменьшают. Таким образом, $\eta$ становится функцией от номера итерации $k$.
	
	\subsection{Регуляризация}
	
	Есть проблема с обучением весов, которые заставляют модель идеально совпадать с обучающей выборкой. Если черта полностью определяет какой-то класс, потому что она встречается только у него, то её будет дан огромный вес. Веса черт будут пытаться идеально подстроиться под обучающую выборку, моделируя даже факторы шума, случайно скоррелировавшие с классом. Эта проблема называется \textit{переобучением}. Хорошая модель должна хорошо обобщать обучающую выборку к тестовой выборке. Переобученная модель будет иметь слабые показатели обобщения.
	
	Для избежания переобучения, \textit{регуляризация} $R(Q)$ добавляется к функции потерь.
	
	\begin{equation}
	\hat{\theta} = \underset{\theta}{argmax} \text{ } logP(y^{(i)} | x^{(i)}) - \alpha R(\theta)
	\end{equation}
	
	Регуляризация призвана <<наказывать>> модель за слишком большие веса. Таким образом, веса, которые соответствуют обучающей выборке идеально, но при этом используют веса с слишком большими значениями, будут наказаны больше, чем веса, которые соответствуют выборке чуть хуже, но зато имеют меньшие значения веса. 
	
	Есть два распространённых способа посчитать регуляризацию. \textit{L2-регуляризация} - это квадратичная функция весов. Норма L2, $||\theta||_2$ - такая же, как \textit{Евклидово расстояние} вектора $\theta$ от начала координат. Если $\theta$ состоит из $n$ векторов, то:
	
	\begin{equation}
	R(\theta) = ||\theta||_2^2 = \sum_{j=1}^{n} \theta_j^2
	\end{equation}
	
	Тогда функция потерь становится такой:
	
	\begin{equation}
	\hat{\theta} = \underset{\theta}{argmax} \text{ } logP(y^{(i)} | x^{(i)}) - \alpha \sum_{j=1}^{n} \theta_j^2
	\end{equation}
	
	\textit{L1-регуляризация} - линейная функция от значений весов, названная так по L1 норме $||W||_1$, сумме модулей величин весов, также известна как \textit{Манхэттенское расстояние}.
	
	\begin{equation}
	R(\theta) = ||\theta||_1 = \sum_{i=1}^n |\theta_j|
	\end{equation}   
	
	Подставленное в функцию потерь:
	
	\begin{equation}
	\hat{\theta} = \underset{\theta}{argmax} \text{ } logP(y^{(i)} | x^{(i)}) - \alpha \sum_{j=1}^{n} |\theta_j|
	\end{equation}
	
	L1 регуляризацию в статистике часто называют \textit{lasso regression}, L2 - \textit{ridge regression}. L2 предпочитает, чтобы в модели было много  маленьких весов, L1 предпочитает разреженные решения с большими весами, но множеством нулевых весов.
	
	
	\section{Векторные представления и семантики}
	
	Роль контекста важна в представлении слов. Слова, которые появляются в похожих контекстах чаще имеют похожий смысл. Эту связь называют $гипотезой распределения$ ($distributional hypothesis$). В этой главе будут представлены векторные семантики, которые основываются на этой лингвистической гипотезе, обучаются представлениям значений слов, которые названы \textit{эмбеддингами}, непосредственно из текстов. 
	
	
	\subsection{Векторные семантики}
	
	Идея векторных семантик заключается в том, чтобы представить слово в виде точки на многоразмерном пространстве семантик. Векторы для представления обычно называют \textit{эмбеддингами}. 
	
	
	\begin{center}
		\includegraphics[scale=0.6]{sema.PNG}
		\\ Рисунок 11 -- двумерная проекция эмбеддингов некоторых слов и фраз, показывающая слова с похожими значениями рядом в пространстве.
	\end{center}
	
	Можно заметить, что положительные и отрицательные слова расположены в разных частях рисунка.
	Векторные семантики очень полезны на практике тем, что они могут быть обучены автоматически из текста без каких-либо сложных обозначений или учителя.
	
	Векторные семантики являются стандартным способом представления слов в NLP.
	
	
	\subsection{Слова и векторы}
	
	Слова или модели распределения значений обычно основаны на \textit{матрице совмещения} (\textit{co-occurrence matrix}).
	
	Один из видов таких матриц - \textit{матрица терминов-документов}. Каждая строка обозначает слово в словаре, каждый столбец - документ, откуда слово было взято. Далее имет пример для четырёх пьес Шекспира.
	
	
	\begin{center}
		\includegraphics[scale=0.6]{shake.PNG}
		\\ Рисунок 12 -- матрица терминов-документов для четырёх слов и четырёх пьес Шекспира
	\end{center}
	
	Можно представлять вектор для документа как точка в |V|-мерном пространстве. Далее представлена проекция матрицы на  двумерную плоскость.
	
	\begin{center}
		\includegraphics[scale=0.6]{proje.PNG}
		\\ Рисунок 13 -- визуализация с измерениями, соответствующими словам \textit{battle} и \textit{fool}
	\end{center}
	
	Матрица терминов-документов изначально была задуман для задачи информационного поиска, то есть, поиска документа $d$ из множества $D$ документов, который лучше всего подходит под запрос $q$.
	
	
	\subsection{Слова как векторы}
	
	Векторные семантики могут быть использованы для представления значения слов, ставя каждому слову в соответствие вектор.
	
	Вместо матрицы терминов-документов используются \textit{матрицы терминов-терминов}, чаще называемые \textit{матрицами слов-слов} или \textit{матрицами терминов-контекста} (\textit{term-term matrix}, \textit{word-word matrix}, \textit{term-context matrix}).
	 Это матрица размерности $|V| \times |V|$? каждая ячейка которой хранит количество раз слово в строке (целевое слово) и слово в столбце (контекст) встречались в каком-то контексте в одном корпусе.
	 
	 \subsection{Косинус для измерения расстояния между словами}
	 
	Для того, чтобы оценить похожесть двух целевых слов, нужна функция от двух слов, показывающая, насколько они близки по значению. Самая часто встречающаяся метрика - косинус угла между векторами.
	
	Косинус основан на скалярном произведении:
	
	\begin{equation}
	\textit{скалярное произведение} (v, w) = v \cdot w = \sum_{i=1}^{N}v_i w_i = v_1 w_1 + v_2 w_2 + \dots + v_N w_N
	\end{equation}
	
	В таком виде метрика имеет проблему: она предпочитает длинные вектора. Длина вектора определяется как:
	
	\begin{equation}
	|v| = \sqrt{\sum_{i=1}^N v_i^2}
	\end{equation}
	
	Самый простой способ модифицировать - это взять \textit{нормированное скалярное произведение}. 
	
	\begin{equation}
	a \cdot b = |a| |b| cos \theta
	\end{equation}
	
	\begin{equation}
	\dfrac{a \cdot b}{|a| |b|} = cos \theta
	\end{equation}
	
	Тогда расстояние между двумя словами может быть вычислено как:
	
	\begin{equation}
	cosine(v, w) = \dfrac{v \cdot w}{|v| |w|} = \dfrac{\sum_{i=1}^N v_i w_i}{\sqrt{\sum_{i=1}^N v_i^2}\sqrt{\sum_{i=1}^N w_i^2}}
	\end{equation}
	
	Иногда используют уже нормализированные векторы, то есть длины 1. Для нормализированных векторов косинус равен скалярному произведению.
	
	
	\subsection{TF-IDF: веса в терминах векторов}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\begin{thebibliography}{3}
		\bibitem{1}
		Фридл, Дж. Регулярные выражения = Mastering Regular Expressions. - СПб.: <<Питер>> , 2001. - 352 с. - (Библиотека программиста). - ISBN 5-318-00056-8.
		
		\bibitem{2}
		Lovins, Julie Beth. Development of a Stemming Algorithm // Mechanical Translation and Computational Linguistics. - 1968. - Т. 11.
		
		\bibitem{3}
		Маннинг К., Рагхаван П., Шютце Х. Введение в информационный поиск. - Вильямс, 2011. - 512 с. - ISBN 978-5-8459-1623-5.
		
		\bibitem{4}
		Crystal, David. A First Dictionary of Linguistics and Phonetics. Boulder, CO: Westview, 1980. Print.
		
		\bibitem{5}
		Ian H. Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes. New York: Van Nostrand Reinhold, 1994. ISBN 978-0-442-01863-4.
		
		\bibitem{6}
		P. Willett. The Porter stemming algorithm: then and now (англ.) // Program: Electronic Library and Information Systems. 2006. Vol. 40, iss. 3. P. 219?223. ? ISSN 0033-0337.
		
		\bibitem{7}
		В. И. Левенштейн. Двоичные коды с исправлением выпадений, вставок и замещений символов. Доклады Академий Наук СССР, 1965. 163.4:845-848.
		
		\bibitem{8}
		Jurafsky, D. and Martin, J.H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Pearson Prentice Hall, 2009. ? 988 p. ? ISBN 9780131873216.
		
		\bibitem{9}
		Proceedings of the ITAT 2008, Information Technologies Applications and Theory, Hrebienok, Slovakia, pp. 23-26, September 2008. ISBN 978-80-969184-8-5
		
		\bibitem{10}
		Bayes, T. (1763). An Essay Toward
		Solving a Problem in the Doctrine
		of Chances, Vol. 53. Reprinted in
		Facsimiles of Two Papers by Bayes,
		Hafner Publishing, 1963
		
		\bibitem{11}
		Mosteller, F. and Wallace, D. L.
		(1964). Inference and Disputed Authorship: The Federalist. SpringerVerlag. A second edition appeared in
		1984 as Applied Bayesian and Classical Inference.
		
		
	\end{thebibliography}	
	%\chapter{список использованных источников}
	
	
\end{document}

